         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle;text-align:center">
                <h3>Preprints</h3>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
              <tr>
                <td class="thumbnail" style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/Arxiv23C.png" alt="Arxiv23B" width="160" height="120" style="border-style: none">
                  <!-- <span class="btnDemo"><a href="http://104.248.15.243/">Demo Link</a></span> -->
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2305.12351.pdf" id="Arxiv23D">
                    <papertitle>Are Your Explanations Reliable? Investigating the Stability of LIME in Explaining Textual Classification Models via Adversarial Perturbation</papertitle>
                  </a>
                  <br>
                  Christopher Burger, Lingwei Chen, Thai Le
                  <br>
                  <i>Preprint, 2023</i>
                  <br>
                  <p class="abstract">Local Surrogate models have increased in popularity for use in explaining complex black-box models for diverse types of data, including text, tabular, and image. One particular algorithm, LIME, continues to see use within the field of machine learning due to its inherently interpretable explanations and model-agnostic behavior. But despite continued use, questions about the stability of LIME persist. Stability, a property where similar instances result in similar explanations, has been shown to be lacking in explanations generated for tabular and image data, both of which are continuous domains. Here we explore the stability of LIME's explanations generated on textual data and confirm the trend of instability shown in previous research for other data types.</p>
                </td>
              </tr>

              <tr>
                <td class="thumbnail" style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/Arxiv23C.png" alt="Arxiv23B" width="160" height="120" style="border-style: none">
                  <!-- <span class="btnDemo"><a href="http://104.248.15.243/">Demo Link</a></span> -->
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/pdf/2303.10430.pdf" id="Arxiv23C">
                    <papertitle>NoisyHate: Benchmarking Content Moderation Machine Learning Models with Human-Written Perturbations Online</papertitle>
                  </a>
                  <br>
                  Yiran Ye, Thai Le, Dongwon Lee
                  <br>
                  <i>Preprint, 2023</i>
                  <br>
                  <p class="abstract">We introduce a benchmark test set containing human-written perturbations online for toxic speech detection models. We test several spell corrector algorithms on this dataset. We also test this data on state-of-the-art language models, such as BERT and RoBERTa, and black box APIs, such as perspective API, to demonstrate the adversarial attack with real human-written perturbations is still effective</p>
                </td>
              </tr>

               <tr>
                <td class="thumbnail" style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/Arxiv23B.png" alt="Arxiv23B" width="160" height="120" style="border-style: none">
                  <!-- <span class="btnDemo"><a href="http://104.248.15.243/">Demo Link</a></span> -->
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2211.09717" id="Arxiv23B">
                    <papertitle>UPTON: Unattributable Authorship Text via Data Poisoning</papertitle>
                  </a>
                  <br>
                  Ziyao Wang, Thai Le, Dongwon Lee
                  <br>
                  <i>Preprint, 2023</i>
                  <br>
                  <p class="abstract">This work proposes UPTON. UPTON uses data poisoning to destroy the authorship feature only in training samples by perturbing them, and try to make released textual data unlearnable on deep neuron networks. It is different from previous obfuscation works, that use adversarial attack to modify the test samples and mislead an AA model, and also the backdoor works, which use trigger words both in test and training samples and only change the model output when trigger words occur</p>
                </td>
              </tr> 

              <tr>
                <td class="thumbnail" style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/reprint23A.png" alt="KDD20" width="160" height="120" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://assets.researchsquare.com/files/rs-2409910/v1/5f2f6e7ffccc2d7a28724ce6.pdf" id="Springer">
                    <papertitle>A Policy-Graph Approach to Explain Reinforcement Learning Agents: A Novel Policy-Graph Approach with Natural Language and Counterfactual Abstractions for Explaining Reinforcement Learning Agents</papertitle>
                  </a>
                  <!-- <a href="https://github.com/lethaiq/GRACE_KDD20">[code]</a> -->
                  <br>
                  Tongtong Liu, Joe McCalmon, Thai Le, Dongwon Lee, Sarra Alqahtani
                  <br>
                  <i>Preprint, 2023</i>
                  <br>
                  <p class="abstract">We propose a novel approach that summarizes an agentâ€™s policy in the form of a directed graph with natural language descriptions with counterfactual explanations.</p>
                </td>
              </tr>

              <tr>
                <td class="thumbnail" style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/Arxiv17B.png" alt="Arxiv17" width="160" height="120" style="border-style: none">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/1710.01977" id="Arxiv17">
                    <papertitle>Machine Learning Based Detection of Clickbait Posts in Social Media</papertitle>
                  </a>
                  <br>
                  Xinyue Cao, Thai Le, Jason(Jiasheng) Zhang
                  <br>
                  <i><em><b>Arxiv</b></em>, 2017</i>
                  <br>
                  <p class="abstract">This work attempts to build an effective computational model to detect clickbaits on Twitter as part of the <a href="https://webis.de/events/clickbait-challenge/">Clickbait Challenge 2017</a></p>
                </td>
              </tr>


          </tbody>
        </table>
